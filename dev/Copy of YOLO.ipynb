{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1oroftPWFvN15gMEdnt0j3AExaSi1KYgo","timestamp":1742276335143}],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_GpUQhJt1rRB"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","import urllib.request\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ywuA5u934t5","executionInfo":{"status":"ok","timestamp":1742156910742,"user_tz":420,"elapsed":2104,"user":{"displayName":"Sheryl Chen","userId":"05424583264081228281"}},"outputId":"0ef7aeef-97e0-44c7-e7ea-694f0c8946ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","\n","def video_to_frames(video_path, output_folder=None, return_frames=True):\n","    \"\"\"\n","    Converts a video file to an array of image frames.\n","\n","    Parameters:\n","    - video_path: Path to the input video file\n","    - output_folder: Optional folder to save the extracted frames as image files\n","    - return_frames: Whether to return the frames as a numpy array\n","\n","    Returns:\n","    - frames: List of numpy arrays representing each frame (if return_frames=True)\n","    \"\"\"\n","\n","    if not os.path.isfile(video_path):\n","        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n","\n","    if output_folder is not None and not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    cap = cv2.VideoCapture(video_path)\n","\n","    if not cap.isOpened():\n","        raise Exception(f\"Error opening video file: {video_path}\")\n","\n","    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","    print(f\"Video properties: {width}x{height}, {fps} fps, {frame_count} frames\")\n","\n","    frames = [] if return_frames else None\n","\n","    for i in tqdm(range(frame_count), desc=\"Processing frames\"):\n","        ret, frame = cap.read()\n","\n","        if not ret:\n","            print(f\"Warning: Could only read {i} frames out of {frame_count}\")\n","            break\n","\n","        if return_frames:\n","            frames.append(frame)\n","\n","        if output_folder is not None:\n","            frame_filename = os.path.join(output_folder, f\"frame_{i:06d}.jpg\")\n","            cv2.imwrite(frame_filename, frame)\n","\n","    cap.release()\n","\n","    return frames\n","\n","\n","# function to create video from a folder of images\n","def images_to_video(image_folder, output_path, fps=30, codec='mp4v', pattern=\"frame_*.jpg\"):\n","    \"\"\"\n","    Converts a folder of images to a video file.\n","\n","    Parameters:\n","    - image_folder: Path to the folder containing image files\n","    - output_path: Path to save the output video\n","    - fps: Frames per second for the output video\n","    - codec: FourCC codec code (default: mp4v for .mp4 files)\n","    - pattern: Glob pattern to match image files\n","\n","    Returns:\n","    - output_path: Path to the saved video file\n","    \"\"\"\n","    import glob\n","\n","    image_files = sorted(glob.glob(os.path.join(image_folder, pattern)))\n","\n","    if not image_files:\n","        raise ValueError(f\"No images found in {image_folder} matching pattern {pattern}\")\n","\n","    first_image = cv2.imread(image_files[0])\n","    height, width = first_image.shape[:2]\n","\n","    output_dir = os.path.dirname(output_path)\n","    if output_dir and not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    fourcc = cv2.VideoWriter_fourcc(*codec)\n","    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n","\n","    for image_file in tqdm(image_files, desc=\"Writing video\"):\n","        frame = cv2.imread(image_file)\n","        out.write(frame)\n","\n","    out.release()\n","\n","    print(f\"Video saved to {output_path}\")\n","    return output_path"],"metadata":{"id":"QR0bXsJw5Qau"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/abewley/sort.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMuKNORrD4x9","executionInfo":{"status":"ok","timestamp":1742156281757,"user_tz":420,"elapsed":848,"user":{"displayName":"Sheryl Chen","userId":"05424583264081228281"}},"outputId":"7e13eac7-24e9-41d8-eec9-de56fc1b0417"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'sort'...\n","remote: Enumerating objects: 208, done.\u001b[K\n","remote: Counting objects: 100% (5/5), done.\u001b[K\n","remote: Compressing objects: 100% (4/4), done.\u001b[K\n","remote: Total 208 (delta 2), reused 1 (delta 1), pack-reused 203 (from 2)\u001b[K\n","Receiving objects: 100% (208/208), 1.20 MiB | 7.21 MiB/s, done.\n","Resolving deltas: 100% (74/74), done.\n"]}]},{"cell_type":"code","source":["import matplotlib\n","\n","matplotlib.use('Agg')\n","\n","import matplotlib.pyplot as plt"],"metadata":{"id":"j1nlH735GnLZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install filterpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_3Vt9YA3k94I","executionInfo":{"status":"ok","timestamp":1742156290001,"user_tz":420,"elapsed":4937,"user":{"displayName":"Sheryl Chen","userId":"05424583264081228281"}},"outputId":"4394b8cf-d1fd-4fd4-8e4e-8250b54b8119"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting filterpy\n","  Downloading filterpy-1.4.5.zip (177 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m203.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from filterpy) (1.26.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from filterpy) (1.14.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from filterpy) (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->filterpy) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.17.0)\n","Building wheels for collected packages: filterpy\n","  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110458 sha256=bb85b6b81067bcd31542ae58ae91d5ce803acb4344ffaf3fbf93e20f21d2f204\n","  Stored in directory: /root/.cache/pip/wheels/12/dc/3c/e12983eac132d00f82a20c6cbe7b42ce6e96190ef8fa2d15e1\n","Successfully built filterpy\n","Installing collected packages: filterpy\n","Successfully installed filterpy-1.4.5\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D_NM21sXnGNW","executionInfo":{"status":"ok","timestamp":1742156372092,"user_tz":420,"elapsed":102,"user":{"displayName":"Sheryl Chen","userId":"05424583264081228281"}},"outputId":"f8dc4b32-941f-4d70-a6b2-1df5e9cad286"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append(\"./sort\")\n","from sort import *\n"],"metadata":{"id":"rLSHaFqxFO-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ultralytics\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpFfCbSvldRi","executionInfo":{"status":"ok","timestamp":1742156552264,"user_tz":420,"elapsed":78177,"user":{"displayName":"Sheryl Chen","userId":"05424583264081228281"}},"outputId":"b8752835-70bd-49f2-cbde-2b9c5ebe3d47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.91-py3-none-any.whl.metadata (35 kB)\n","Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.91-py3-none-any.whl (949 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.2/949.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.91 ultralytics-thop-2.0.14\n"]}]},{"cell_type":"code","source":["!pip install easyocr"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vRkP7g4XmGGu","executionInfo":{"status":"ok","timestamp":1742156569438,"user_tz":420,"elapsed":3420,"user":{"displayName":"Sheryl Chen","userId":"05424583264081228281"}},"outputId":"bc1d791d-1aa5-4ef7-9ef9-b8a16bcb58b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting easyocr\n","  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.21.0+cu124)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.11.0.86)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.14.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.26.4)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from easyocr) (11.1.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.2)\n","Collecting python-bidi (from easyocr)\n","  Downloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from easyocr) (6.0.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.7)\n","Collecting pyclipper (from easyocr)\n","  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n","Collecting ninja (from easyocr)\n","  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.2.18)\n","Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (24.2)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n","Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_bidi-0.6.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (292 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.9/292.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n","Successfully installed easyocr-1.7.2 ninja-1.11.1.3 pyclipper-1.3.0.post6 python-bidi-0.6.6\n"]}]},{"cell_type":"code","source":["def frames_to_video(frames, output_video, fps=30):\n","    \"\"\"\n","    Convert a list of frames to a video file.\n","\n","    Parameters:\n","    - frames: List of processed image frames\n","    - output_video: Path where the video will be saved\n","    - fps: Frames per second (default: 30)\n","    \"\"\"\n","    if not frames:\n","        print(\"No frames to process\")\n","        return\n","\n","    # Get dimensions from first frame\n","    height, width = frames[0].shape[:2]\n","\n","    # Define the codec and create VideoWriter object\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or use 'XVID' for .avi\n","    out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n","\n","    try:\n","        # Write frames to video\n","        for frame in tqdm(frames, desc=\"Writing video\"):\n","            out.write(frame)\n","\n","        print(f\"Video successfully saved to: {output_video}\")\n","\n","    except Exception as e:\n","        print(f\"Error writing video: {str(e)}\")\n","\n","    finally:\n","        # Release the video writer\n","        out.release()\n","\n","# Example usage:\n","# frames_to_video(processed_frames, \"output.mp4\", fps=30)"],"metadata":{"id":"eoZF87bIv7aR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ultralytics import YOLO\n","import easyocr"],"metadata":{"id":"mxCQ4icilipx","executionInfo":{"status":"ok","timestamp":1742156582775,"user_tz":420,"elapsed":9585,"user":{"displayName":"Sheryl Chen","userId":"05424583264081228281"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc601899-1da1-4508-f5fa-9ea6ef17c4f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}]},{"cell_type":"code","source":["import os\n","print(os.listdir(\"/content/sort\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pm0kFasVoYmQ","executionInfo":{"status":"ok","timestamp":1742156697122,"user_tz":420,"elapsed":5,"user":{"displayName":"Sheryl Chen","userId":"05424583264081228281"}},"outputId":"e0b429fc-7fde-4537-fd77-3d051f1aace1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['sort.py', 'data', 'README.md', 'LICENSE', '.git', '.gitignore', 'requirements.txt']\n"]}]},{"cell_type":"code","source":["!pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N5gm3RkiwfZY","executionInfo":{"status":"ok","timestamp":1742158826504,"user_tz":420,"elapsed":2214,"user":{"displayName":"Sheryl Chen","userId":"05424583264081228281"}},"outputId":"a1b2f619-3c90-4997-e67f-bed239d90d34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"]}]},{"cell_type":"code","source":["import importlib.util\n","import torch\n","spec = importlib.util.spec_from_file_location(\"sort\", \"/content/sort/sort.py\")\n","sort_module = importlib.util.module_from_spec(spec)\n","spec.loader.exec_module(sort_module)\n","\n","Sort = sort_module.Sort  # Access the Sort class\n","# Load YOLOv8 model\n","# Add GPU support\n","model = YOLO('yolov8n.pt').to('cuda' if torch.cuda.is_available() else 'cpu')  # or 'yolov8n.pt' for smaller/faster model\n","reader = easyocr.Reader(['en'])\n"],"metadata":{"id":"VXew8IhAoE9A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def download_file(url, filename):\n","    if not os.path.exists(filename):\n","        print(f\"Downloading {filename}...\")\n","        urllib.request.urlretrieve(url, filename)\n","        print(f\"Downloaded {filename}\")\n","    else:\n","        print(f\"{filename} already exists\")\n","\n","def process_frames_with_tracking(input_folder, output_video, max_frames=60):\n","    \"\"\"\n","    Process frames with YOLOv8 to detect people, track them across frames,\n","    and create a video with assigned IDs.\n","\n","    Parameters:\n","    - input_folder: Folder containing the input frames\n","    - output_video: Path to save the output video\n","    - max_frames: Maximum number of frames to process\n","    \"\"\"\n","\n","    # Initialize SORT tracker\n","    tracker = Sort()\n","\n","    # Get frame files\n","    frame_files = sorted([f for f in os.listdir(input_folder) if f.startswith(\"frame_\")])[:max_frames]\n","    if not frame_files:\n","        raise ValueError(f\"No frames found in {input_folder}\")\n","\n","    processed_frames = []\n","\n","    for frame_file in tqdm(frame_files, desc=\"Processing frames with YOLOv8 and tracking\"):\n","        frame_path = os.path.join(input_folder, frame_file)\n","        image = cv2.imread(frame_path)\n","        if image is None:\n","            print(f\"Warning: Could not read {frame_path}\")\n","            continue\n","\n","        # Run YOLOv8 inference\n","        results = model(image, conf=0.5)  # Increased confidence threshold for tracking\n","\n","        # Process detections\n","        detections = []\n","\n","        for result in results:\n","            boxes = result.boxes\n","            for box in boxes:\n","                # Get box coordinates, confidence, and class\n","                x1, y1, x2, y2 = map(int, box.xyxy[0])\n","                conf = float(box.conf[0])\n","                cls = int(box.cls[0])\n","\n","                # Check if detection is person (class 0 in COCO)\n","                if cls == 0:  # person class\n","                    # Format detection for SORT: [x1, y1, x2, y2, confidence]\n","                    detections.append([x1, y1, x2, y2, conf])\n","\n","        # Convert detections to NumPy array\n","        if len(detections) > 0:\n","            detections = np.array(detections)\n","        else:\n","            detections = np.empty((0, 5))\n","\n","        # Update tracker\n","        tracked_objects = tracker.update(detections)\n","\n","        # Draw tracking results\n","        for obj in tracked_objects:\n","            x1, y1, x2, y2, track_id = obj.astype(int)\n","\n","            # Draw bounding box\n","            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","\n","            # Draw ID and try to detect jersey number in this region\n","            cv2.putText(image, f\"ID: {track_id}\", (x1, y1 - 10),\n","                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","\n","            # Optional: Extract ROI for jersey number detection\n","            roi = image[y1:y2, x1:x2]\n","            if roi.size > 0:  # Check if ROI is valid\n","                # Convert to grayscale for better number detection\n","                gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n","\n","                # Apply thresholding to isolate numbers\n","                _, thresh = cv2.threshold(gray_roi, 127, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n","\n","                # Try to detect numbers using EasyOCR\n","\n","                results = reader.readtext(\n","                    thresh,\n","                    allowlist='0123456789',\n","                    paragraph=False,\n","                    min_size=20,\n","                    width_ths=0.5\n","                )\n","\n","                # If numbers are detected, display them\n","                for (bbox, text, prob) in results:\n","                    if text.isdigit() and prob > 0.5:\n","                        cv2.putText(image, f\"#{text}\", (x1, y2 + 20),\n","                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 2)\n","                        break  # Only show first detected number\n","\n","        # Add processed frame to list\n","        processed_frames.append(image)\n","\n","    # Create video from processed frames\n","    if processed_frames:\n","        print(f\"Creating video from {len(processed_frames)} processed frames...\")\n","        frames_to_video(processed_frames, output_video, fps=30)\n","        print(f\"Video saved to {output_video}\")\n","    else:\n","        print(\"No frames were processed successfully.\")\n","\n","import os\n","import cv2\n","import numpy as np\n","from tqdm import tqdm\n","\n","def process_frames_with_bounding_boxes(input_folder, output_video, max_frames=60):\n","    \"\"\"\n","    Process frames with YOLOv8 to detect people, draw bounding boxes,\n","    and create a video from the processed frames.\n","\n","    Parameters:\n","    - input_folder: Folder containing the input frames\n","    - output_video: Path to save the output video\n","    - max_frames: Maximum number of frames to process\n","    \"\"\"\n","    # Load YOLOv8 model\n","    model = YOLO('yolov8s.pt')  # or 'yolov8n.pt' for smaller/faster model\n","\n","    # Get frame files\n","    frame_files = sorted([f for f in os.listdir(input_folder) if f.startswith(\"frame_\")])[:max_frames]\n","    if not frame_files:\n","        raise ValueError(f\"No frames found in {input_folder}\")\n","\n","    processed_frames = []\n","\n","    for frame_file in tqdm(frame_files, desc=\"Processing frames with YOLOv8\"):\n","        # Load the image\n","        frame_path = os.path.join(input_folder, frame_file)\n","        image = cv2.imread(frame_path)\n","        if image is None:\n","            print(f\"Warning: Could not read {frame_path}\")\n","            continue\n","\n","        # Run YOLOv8 inference\n","        results = model(image, conf=0.15)  # confidence threshold 0.25\n","\n","        # Process detections\n","        for result in results:\n","            boxes = result.boxes\n","            for box in boxes:\n","                # Get box coordinates and confidence\n","                x1, y1, x2, y2 = map(int, box.xyxy[0])\n","                conf = float(box.conf[0])\n","                cls = int(box.cls[0])\n","\n","                # Check if detection is person (class 0 in COCO)\n","                if cls == 0:  # person class\n","                    # Draw bounding box\n","                    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","                    cv2.putText(image, f\"Person: {conf:.2f}\", (x1, y1 - 10),\n","                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","\n","        # Add the processed frame to our list\n","        processed_frames.append(image)\n","\n","    # Create video from processed frames\n","    if processed_frames:\n","        print(f\"Creating video from {len(processed_frames)} processed frames...\")\n","        frames_to_video(processed_frames, output_video, fps=30)\n","        print(f\"Video saved to {output_video}\")\n","    else:\n","        print(\"No frames were processed successfully.\")\n","\n","\n","\n","if __name__ == \"__main__\":\n","    video_path = \"/content/drive/MyDrive/CS131 Final Proj/nba_clip2.mov\"\n","    frames_path = \"/content/drive/MyDrive/CS131 Final Proj/Test Frames Video 1\"\n","    # Process the first 60 frames and create a video\n","    #process_frames_with_bounding_boxes(frames_path, video_path, max_frames=60)\n","    process_frames_with_tracking(frames_path, video_path, max_frames=60)"],"metadata":{"id":"N2vq_2my2Bmi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"001c3821-3df8-4a52-8553-05c75a28b1cf","executionInfo":{"status":"ok","timestamp":1742158892745,"user_tz":420,"elapsed":7815,"user":{"displayName":"Sheryl Chen","userId":"05424583264081228281"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:   0%|          | 0/60 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 7 persons, 10.2ms\n","Speed: 3.4ms preprocess, 10.2ms inference, 11.2ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:   2%|▏         | 1/60 [00:00<00:05,  9.95it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 7 persons, 8.1ms\n","Speed: 2.5ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:   3%|▎         | 2/60 [00:00<00:05,  9.96it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.2ms\n","Speed: 2.5ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:   5%|▌         | 3/60 [00:00<00:06,  9.22it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.3ms\n","Speed: 2.6ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 8.4ms\n","Speed: 2.8ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:   8%|▊         | 5/60 [00:00<00:05,  9.88it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 10.1ms\n","Speed: 4.0ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  10%|█         | 6/60 [00:00<00:05,  9.19it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 9 persons, 10.0ms\n","Speed: 4.0ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  12%|█▏        | 7/60 [00:00<00:05,  9.03it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 9 persons, 8.9ms\n","Speed: 3.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  13%|█▎        | 8/60 [00:00<00:05,  9.16it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 9 persons, 8.2ms\n","Speed: 2.6ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  15%|█▌        | 9/60 [00:00<00:05,  9.36it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 9 persons, 7.8ms\n","Speed: 2.5ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  17%|█▋        | 10/60 [00:01<00:05,  9.48it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.8ms\n","Speed: 2.5ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  18%|█▊        | 11/60 [00:01<00:05,  9.40it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.0ms\n","Speed: 2.6ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  20%|██        | 12/60 [00:01<00:05,  9.37it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 7.9ms\n","Speed: 2.7ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  22%|██▏       | 13/60 [00:01<00:05,  9.31it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 9 persons, 8.2ms\n","Speed: 2.9ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  23%|██▎       | 14/60 [00:01<00:04,  9.26it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.2ms\n","Speed: 2.8ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  25%|██▌       | 15/60 [00:01<00:04,  9.23it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.1ms\n","Speed: 2.8ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  27%|██▋       | 16/60 [00:01<00:04,  8.92it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 7.9ms\n","Speed: 2.5ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  28%|██▊       | 17/60 [00:01<00:04,  9.05it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.7ms\n","Speed: 2.7ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  30%|███       | 18/60 [00:01<00:04,  9.21it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 8.2ms\n","Speed: 2.8ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  32%|███▏      | 19/60 [00:02<00:04,  9.27it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 7.9ms\n","Speed: 2.6ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  33%|███▎      | 20/60 [00:02<00:04,  9.21it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 8 persons, 8.1ms\n","Speed: 2.7ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 7.9ms\n","Speed: 2.5ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  37%|███▋      | 22/60 [00:02<00:03,  9.70it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 10.4ms\n","Speed: 2.6ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 8.6ms\n","Speed: 2.5ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  40%|████      | 24/60 [00:02<00:03,  9.83it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 8.4ms\n","Speed: 2.6ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 7.7ms\n","Speed: 2.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  43%|████▎     | 26/60 [00:02<00:03,  9.62it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.1ms\n","Speed: 2.8ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  45%|████▌     | 27/60 [00:02<00:03,  9.50it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.0ms\n","Speed: 2.5ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  47%|████▋     | 28/60 [00:02<00:03,  9.22it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 8.4ms\n","Speed: 2.6ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  48%|████▊     | 29/60 [00:03<00:03,  9.03it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 8.3ms\n","Speed: 2.8ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  50%|█████     | 30/60 [00:03<00:03,  9.01it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.5ms\n","Speed: 2.9ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  52%|█████▏    | 31/60 [00:03<00:03,  9.14it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.1ms\n","Speed: 2.5ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  53%|█████▎    | 32/60 [00:03<00:03,  8.91it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 12 persons, 8.1ms\n","Speed: 2.4ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  55%|█████▌    | 33/60 [00:03<00:03,  8.65it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 12 persons, 8.0ms\n","Speed: 2.5ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  57%|█████▋    | 34/60 [00:03<00:03,  8.45it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 7.9ms\n","Speed: 2.5ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  58%|█████▊    | 35/60 [00:03<00:02,  8.38it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 8.3ms\n","Speed: 2.6ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  60%|██████    | 36/60 [00:03<00:02,  8.15it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 7.9ms\n","Speed: 2.5ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  62%|██████▏   | 37/60 [00:04<00:02,  8.08it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 8.4ms\n","Speed: 2.6ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  63%|██████▎   | 38/60 [00:04<00:02,  8.10it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 8.5ms\n","Speed: 2.8ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  65%|██████▌   | 39/60 [00:04<00:02,  8.12it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 7.9ms\n","Speed: 2.5ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  67%|██████▋   | 40/60 [00:04<00:02,  8.02it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 12 persons, 8.2ms\n","Speed: 2.7ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  68%|██████▊   | 41/60 [00:04<00:02,  7.85it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 12 persons, 8.1ms\n","Speed: 2.9ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  70%|███████   | 42/60 [00:04<00:02,  7.75it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 8.1ms\n","Speed: 2.5ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  72%|███████▏  | 43/60 [00:04<00:02,  7.97it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 8.0ms\n","Speed: 2.6ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  73%|███████▎  | 44/60 [00:04<00:02,  7.99it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 9 persons, 8.3ms\n","Speed: 2.6ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  75%|███████▌  | 45/60 [00:05<00:01,  8.24it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 9 persons, 8.1ms\n","Speed: 2.6ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  77%|███████▋  | 46/60 [00:05<00:01,  8.44it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 8.0ms\n","Speed: 2.6ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  78%|███████▊  | 47/60 [00:05<00:01,  8.46it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.8ms\n","Speed: 3.8ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  80%|████████  | 48/60 [00:05<00:01,  8.41it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 8.0ms\n","Speed: 2.8ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 8.1ms\n","Speed: 2.8ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  83%|████████▎ | 50/60 [00:05<00:01,  9.14it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 10.0ms\n","Speed: 3.9ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  85%|████████▌ | 51/60 [00:05<00:01,  8.61it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 10 persons, 7.7ms\n","Speed: 2.6ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 7.9ms\n","Speed: 2.4ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  88%|████████▊ | 53/60 [00:05<00:00,  8.94it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 11 persons, 8.3ms\n","Speed: 2.5ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  90%|█████████ | 54/60 [00:06<00:00,  8.76it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 8 persons, 8.1ms\n","Speed: 3.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  92%|█████████▏| 55/60 [00:06<00:00,  8.75it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 8 persons, 8.1ms\n","Speed: 2.8ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  93%|█████████▎| 56/60 [00:06<00:00,  8.97it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 6 persons, 8.3ms\n","Speed: 2.8ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 8.0ms\n","Speed: 2.8ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing frames with YOLOv8 and tracking:  97%|█████████▋| 58/60 [00:06<00:00, 10.22it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","0: 384x640 7 persons, 10.1ms\n","Speed: 3.9ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 7.9ms\n","Speed: 2.5ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"stream","name":"stderr","text":["Processing frames with YOLOv8 and tracking: 100%|██████████| 60/60 [00:06<00:00,  9.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Creating video from 60 processed frames...\n"]},{"output_type":"stream","name":"stderr","text":["Writing video: 100%|██████████| 60/60 [00:01<00:00, 54.18it/s]"]},{"output_type":"stream","name":"stdout","text":["Video successfully saved to: /content/drive/MyDrive/CS131 Final Proj/nba_clip2.mov\n","Video saved to /content/drive/MyDrive/CS131 Final Proj/nba_clip2.mov\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"-KaxQL9e9JDe"},"execution_count":null,"outputs":[]}]}